# 3.1 张量、计算图、会话

目标：搭建第一个神经网络，总结搭建八股

基于Tensorflow的NN：用张量表示数据，用计算图搭建神经网络，用会话执行计算图，优化线上的权重（参数），得到模型。

张量(tensor)：多维数组（列表）

阶：张量的维数

| 维数 | 阶   | 名字        | 例子                        |
| ---- | ---- | ----------- | --------------------------- |
| 0-D  | 0    | 标量 scalar | s = 1 2 3                   |
| 1-D  | 1    | 向量 vector | v = [1, 2, 3]               |
| 2-D  | 2    | 矩阵 matrix | m=[[1,2,3],[4,5,6],[7,8,9]] |
| n-D  | n    | 张量 tensor | t=[[[[n个......             |

张量可以表示 0 阶到 n 阶数组（列表）

数据类型： tf.float32 tf.int32 ......

```bash
# vimrc
set ts=4	# tab键为4个空格
set nu		# 显示行号
```

tensorflow加法

```python
import tensorflow as tf # 简写
a = tf.constant([1.0, 2.0])
b = tf.constant([3.0, 4.0])
result = a + b
print result
```

输出

```
Tensor("add:0", shape=(2,), dtype=float32)
         |  |     |    |      |
     节点名  |   维度    |     数据类型
     第0个输出      一维数组长度为2
```



```python
import tensorflow as tf
x = tf.constant([[1.0, 2.0]])
w = tf.constant([[3.0], [4.0]])
y = tf.matmul(x, w)
print y
```



输出

```
Tensor("MatMul:0", shape=(1, 1), dtype=float32)
                      一行一列
```



上述只搭建了计算图，没有运算结果。想要有运算结果就要用到会话。

## 会话

执行计算图中的节点运算

```python
import tensorflow as tf #引入模块
x = tf.constant([[1.0, 2.0]]) #定义一个 2 阶张量等于[[1.0,2.0]]
w = tf.constant([[3.0], [4.0]]) #定义一个 2 阶张量等于[[3.0],[4.0]]
y = tf.matmul(x, w) #实现 xw 矩阵乘法
print y #打印出结果
with tf.Session() as sess:
	print sess.run(y) #执行会话并打印出执行后的结果
```

输出

```
Tensor(“matmul:0”, shape(1,1), dtype=float32)
[[11.]]	
```



```python
tf.Session() # 会话模块，可简写
```

降低tensorflow的提示等级

在 bashrc/zshrc 中加入

```bash
export TF_CPP_MIN_LOG_LEVEL=2
```

# 3.2 前向传播

神经网络的参数：线上的权重W，用变量表示，随机给初值。

```python
w = tf.Variable(tf.random_normal([2, 3], stddev = 2, mean = 0, seed = 1))
#                     |             |      |           |         |
#                  正态分布    产生2x3的矩阵 标准差为2    均值为0   随机种子
tf.truncated_normal() # 去掉过大偏离点的正态分布(两个标准差)
tf.random_uniform()   # 平均分布
tf.zeros # 全0数组 tf.zeros([3, 2], int32)生成[[0, 0], [0, 0], [0, 0]]
tf.ones # 全1数组 tf.ones([3, 2], int32)生成[[1, 1], [1, 1], [1, 1]]
tf.fill # 全定值数组 tf.fill([3, 2], 6)生成[[6, 6], [6, 6], [6, 6]]
tf.constant # 直接给值 tf.constant([3, 2, 1])生成[3, 2, 1]
```

## 神经网络的实现过程

1. 准备数据集，提取特征，作为输入喂给神经网络 (Neural Network, NN)

2. 搭建NN结构，从输入到输出（先打剑计算图，再用会话执行）

    （NN前向传播算法 ---> 计算输出）

3. 大量特征数据喂给NN，迭代优化NN参数

    （NN反向传播算法 ---> 优化参数训练模型）

4. 使用训练好的模型预测和分类

训练过程：步骤1到3的循环迭代

使用过程：步骤4

## 前向传播

搭建模型，实现推理（以全连接网络为例）

例：生产一批零件将体积 $$ x_{1} $$ 和重量 $$ x_{2} $$ 为特征输入NN，通过NN后输出一个数值。

![](3-1.png)

分析：
$$
X是输入为 1\times 2 矩阵；W_{前节点编号，后节点编号^{(层数)}}为待优化参数 \\
W^{(1)} =\begin{bmatrix}
 W_{1,1^{(1)}}&W_{1,2^{(1)}}  &W_{1,3^{(1)}} \\ 
 W_{2,1^{(1)}}&W_{2,2^{(1)}}  &W_{2,3^{(1)}}  
\end{bmatrix} \  \  \  \ 为\ 2\times 3\ 矩阵\\
a^{(1)} = a[a_{11},a_{12},a_{13}]\ 为\ 1\times3\ 矩阵\ = XW^{(1)}\\
W^{(2)} = \begin{bmatrix}
W_{1,1^{(2)}}\\
W_{2,1^{(2)}}\\
W_{3,1^{(2)}}
\end{bmatrix} \ \ \ \ 为\ 3\times 1 \ 矩阵\ \ \ \ y = a^{(1)}W^{(2)}
$$
即

```python
a = tf.matmul(X, W1)
y = tf.matmul(a, W2)
```

代码实现

```python
# 变量初始化、计算图节点的运算过程都要用会话(with结构)实现
with tf.Session as sess:
    sess.run()
# 变量初始化: 在 sess.run 函数中用 tf.global_variables_initializer()
init_op = tf.global_variables_initializer()
sess.run(init_op)
# 计算图节点运算: 在 sess.run 函数中写入待运算的节点
sess.run(y)
# 用 tf.placeholder 占位，在 sess.run 函数中用 feed_dict 喂数据
# 喂一组数据
x = tf.placeholder(tf.float32, shape = (1, 2))
sess.run(y, feed_dict = {x: [[0.5, 0.6]]})
# 喂多组数据
x = tf.placeholder(tf.float32, shape = (None, 2))
sess.run(y, feed_dict = {x: [[0.1, 0.2], [0.2, 0.3], [0.3, 0.4], [0.4, 0.5]]})
# 实践代码 tf3_3.py 感受NN前向传播的过程
```



```python
# tf3_3.py
#coding:utf-8
#两层简单神经网络（全连接）
import tensorflow as tf

#定义输入和参数
x = tf.constant([[0.7, 0.5]])
w1= tf.Variable(tf.random_normal([2, 3], stddev=1, seed=1))
w2= tf.Variable(tf.random_normal([3, 1], stddev=1, seed=1))

#定义前向传播过程
a = tf.matmul(x, w1)
y = tf.matmul(a, w2)

#用会话计算结果
with tf.Session() as sess:
    init_op = tf.global_variables_initializer()
    sess.run(init_op)
    print"y in tf3_3.py is:\n",sess.run(y) 

'''
y in tf3_3.py is : 
[[3.0904665]]
'''

```



