# 4.1 损失函数

![](4-1.png)

![](4-2.png)

引入的激活函数 $$f$$ 可以有效避免仅使用 $$\sum X_{i}W_{i} $$ 的纯线性组合，提高了模型的表达力，使模型具有更好的区分度。

常用的激活函数：

![](4-3.png)

NN复杂度： NN层数和NN参数的个数

## 神经网络的优化

四方面：损失函数loss、学习率learning_rate、滑动平均ema、正则化regularization

### 损失函数(loss) ：预测值(y)与已知答案(y_)的差距

```
                      /--  mse (Mean Squared Error) 均方误差
NN优化目标： loss最小  <---- 自定义
                      \--  ce (Cross Entropy) 交叉熵
```

#### 均方误差 mse

运行 `opt4_1.py` 代码，发现梯度下降优化方法收敛速度慢，20000次还没有完全收敛。

使用 `Momentum优化器` 优化，过了6500轮就收敛了。

而 `Adam优化器` 的效果也比较不错

#### 自定义损失函数

 实现方法

```python
loss = tf.reduce_sum(tf.where(tf.greater(y,y_),COST(y-y_),PROFIT(y_-y)))
# tf.greater(y, y_), COST(y - y_), PROFIT(y_ - y)
# y > y_ ? COST(y - y_) : PROFIT(y_ - y)
```

### 交叉熵 ce(Cross Entropy)

表征两个概率分布之间的距离

```python
ce = -tf.reduce_mean(y_ * tf.log(tf.clip_by_value(y, 1e-12, 1.0)))
# tf.clip_by_value(y, 1e-12, 1.0)
# y 小于 1e-12 为 1e-12， 大于 1.0 为 1.0
```

```python
ce = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=y, labels=tf.argmax(y_, 1))
cem = tf.reduce_mean(ce)
# 输出为当前计算出的预测值与标准答案的差距，即损失函数
```

# 4.2 学习率 learning rate

参数每次更新的幅度
$$
w_{n-1}=w_{n}- \ learning\_rate\bigtriangledown
$$
其中
$$
w_{n+1}为更新后的参数\\
w_{n} 为当前参数\\
learnng_rate 为学习率\\
\bigtriangledown 为损失函数的梯度(导数)\\
即参数更新向着损失函数梯度下降的方向
$$

## 学习率设置多少合适？

学习率大了不收敛，学习率小了收敛速度慢

### 指数衰减学习率

根据运行 `BATCH_SIZE` 的轮数动态更新学习率
$$
learning\_rate = LEARNING\_RATE\_BASE*LEARNING\_RATE\_DECAY^{\frac{global\_step}{LEARNING\_RATE\_STEP}}\\
LEARNING\_RATE\_BASE : \ \ 学习率基数，学习率初始值\\
LEARNING\_RATE\_DECAY : \ \ 学习率衰减率(0,\ 1)\\
global\_step : \ \ 运行了几轮 BATCH\_SIZE\\
LEARNING\_RATE\_STEP : \ \ 多少轮更新一次学习率 = 总样本数 / BATCH\_SIZE
$$


```python
# 首先定义一个 global_step 作为计数器记录当前共运行了多少轮 BATCH_SIZE 个数据
global_step = tf.Variable(0, trainable = False)
# 由于这个变量只用于计数，并非训练的参数，因此我们标注它为不可训练

learning_rate = tf.train.exponentual_decay(
	LEARNING_RATE_BASE, # 学习率基数, 最初的学习率, 超参数
	global_step, # 当前运行到第几轮的计数器
	LEARNING_RATE_STEP, # 学习率多少轮更新一次
	LEARNING_RATE_DECAY,
	staircase = True) 
# staircase 为 True 时 global_step / LEARNING_RATE_STEP 取整数，学习率阶梯型衰减
#           为 False 时则为一条平滑下降的曲线
```





